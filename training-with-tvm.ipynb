{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training some neural net on mnist with pure tvm\n",
    "\n",
    "Here we are going to train a neural net in pure tvm (no nnvm) using tvm-level automatic differentiation. We also implement the same network in keras and use it as a reference implementation. Some operations, like max pooling and flatten are not fully supported yet, so we avoid using them. Also note that we do little scheduling here, and since we don't use nnvm, some optimization are not performed, so it's all going to be quite slow. In real life, tvm-level autodiff is supposed to be used on individual nnvm/relay operations together with some kind of automatic scheduling, may be autotuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topi\n",
    "import tvm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data using keras utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/p3i9s3vhjskbrnfl97fd7b0vmn7bqddh-python3.6-h5py-2.7.1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch generator. The last incomplete batch is thrown out because we use fixed batch size. We will use the same function for keras so that the training results are closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x, y):\n",
    "    for i in range(int(x.shape[0] / batch_size)):\n",
    "        yield (x[i:i+batch_size, :, :, None].astype('float32'),\n",
    "               y[i:i+batch_size, ...].astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "This is the keras definition of the model. Note that we avoid max pooling and flattening because our autodiff implementation doesn't fully support these operations (but we are working on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keras_model():\n",
    "    data = keras.layers.Input(shape=(28, 28, 1))\n",
    "    x = data\n",
    "    x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    # We don't support max pooling, so use average pooling\n",
    "    x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "    # We don't support flatten, so we rewrite flatten+dense into conv2d+squeeze\n",
    "    x = keras.layers.Conv2D(128, (12, 12), activation='relu')(x)\n",
    "    x = keras.layers.Flatten()(x) # this will become squeeze in tvm\n",
    "    x = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    keras_model = keras.models.Model(data, x)\n",
    "\n",
    "    keras_model.compile(loss=keras.losses.categorical_crossentropy, metrics=['accuracy'],\n",
    "                        optimizer=keras.optimizers.SGD(lr=1e-2))\n",
    "    \n",
    "    return keras_model\n",
    "\n",
    "keras_model = make_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 128)         1179776   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same thing written in tvm. Note that we use a custom implementation of softmax because the topi implementation is too complex for our automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grechanik/proj/mytvm/python/tvm/tag.py:32: UserWarning: Tag 'injective' declared via TagScope was not used.\n",
      "  warnings.warn(\"Tag '%s' declared via TagScope was not used.\" % (self.tag,))\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "\n",
    "x = tvm.placeholder((batch_size, 28, 28, 1))\n",
    "y = tvm.placeholder((batch_size, num_classes))\n",
    "\n",
    "t = topi.transpose(x, [0, 3, 1, 2])\n",
    "\n",
    "w1 = tvm.placeholder((32, 1, 3, 3), name=\"w1\")\n",
    "b1 = tvm.placeholder((32,), name=\"b1\")\n",
    "t = topi.nn.relu(topi.nn.conv2d(t, w1, 1, 0) + topi.reshape(b1, (1, 32, 1, 1)))\n",
    "weights.extend([w1, b1])\n",
    "\n",
    "w2 = tvm.placeholder((64, 32, 3, 3), name=\"w2\")\n",
    "b2 = tvm.placeholder((64,), name=\"b2\")\n",
    "t = topi.nn.relu(topi.nn.conv2d(t, w2, 1, 0) + topi.reshape(b2, (1, 64, 1, 1)))\n",
    "weights.extend([w2, b2])\n",
    "\n",
    "t = topi.nn.pool(t, [2, 2], [2, 2], [0, 0, 0, 0], 'avg')\n",
    "\n",
    "w3 = tvm.placeholder((128, 64, 12, 12), name=\"w3\")\n",
    "b3 = tvm.placeholder((128,), name=\"b3\")\n",
    "t = topi.nn.relu(topi.nn.conv2d(t, w3, 1, 0) + topi.reshape(b3, (1, 128, 1, 1)))\n",
    "weights.extend([w3, b3])\n",
    "\n",
    "# Note that we have to transpose before flatten\n",
    "#t = topi.transpose(t, [0, 2, 3, 1])\n",
    "#t = topi.nn.flatten(t)\n",
    "\n",
    "# Squeeze instead of flatten\n",
    "t = topi.squeeze(t)\n",
    "\n",
    "w4 = tvm.placeholder((num_classes, np.prod([s.value for s in t.shape[1:]])), name=\"w4\")\n",
    "b4 = tvm.placeholder((num_classes,), name=\"b4\")\n",
    "t = topi.nn.dense(t, w4, b4)\n",
    "weights.extend([w4, b4])\n",
    "\n",
    "# We use a custom softmax because the topi implementation uses max behind the scenes\n",
    "# which currently causes problems with autodiff, leading to poor performance\n",
    "exps = topi.exp(t)\n",
    "sumexps = topi.sum(exps, axis=-1, keepdims=True)\n",
    "logsoftmax = topi.log(exps/sumexps)\n",
    "\n",
    "predictions = topi.nn.softmax(t)\n",
    "loss = - topi.sum(y * logsoftmax) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform automatic differentiation of the loss wrt the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head is just the derivative of loss wrt itself which is just one\n",
    "head = topi.full((1,), 'float32', 1.0)\n",
    "\n",
    "# JacobianRecursive performs reverse-mode automatic differentiation.\n",
    "# It is called \"Jacobian\", but we'll get gradients because loss is a scalar.\n",
    "gradients = list(tvm.ir_pass.JacobianRecursive(loss, weights, head))\n",
    "\n",
    "# We feed the learning rate as an input.\n",
    "learning_rate = tvm.placeholder(())\n",
    "\n",
    "# For simplicity we compute new weights as an output and then copy the result to the input\n",
    "new_weights = [w - learning_rate*g for w, g in zip(weights, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and initializing the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor's shape as a list\n",
    "def get_shape(tensor):\n",
    "    return [s.value for s in tensor.shape]\n",
    "\n",
    "# empty tensor values for a list of tensors or just a single tensor\n",
    "def empty_val(tensor):\n",
    "    if isinstance(tensor, list):\n",
    "        return [empty_val(t) for t in tensor]\n",
    "    else:\n",
    "        return tvm.nd.empty(get_shape(tensor), tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just assume that we have 20 cores and parallelize it somehow. (TODO: find something better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_somehow(sched):\n",
    "    # This autoinlining turned aoyt to be harmful because it prevents memoization of some expensive operations\n",
    "    # tvm.schedule.AutoInlineInjective(sched)\n",
    "    for s in sched.stages:\n",
    "        if isinstance(s.op, tvm.tensor.ComputeOp) and isinstance(s.op.body[0], tvm.expr.Reduce):\n",
    "            ax = s.fuse(*s.op.axis)\n",
    "            axo, axi = s.split(ax, nparts=20)\n",
    "            s.parallel(axo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two separate modules: one for testing and one for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = tvm.create_schedule(loss.op)\n",
    "schedule_somehow(sched)\n",
    "testing_module = tvm.build(sched, [loss, x, y] + weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = tvm.create_schedule([loss.op] + [w.op for w in new_weights])\n",
    "schedule_somehow(sched)\n",
    "training_module = tvm.build(sched, [loss, x, y, learning_rate] + new_weights + weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class just stores the current state of weights and provides a couple of useful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TvmModel:\n",
    "    def __init__(self):\n",
    "        self.weights_values = empty_val(weights)\n",
    "        \n",
    "    def test(self, xval, yval):\n",
    "        args = [empty_val(loss)] + [tvm.ndarray.array(xval), tvm.ndarray.array(yval)] + self.weights_values\n",
    "        testing_module(*args)\n",
    "        return args[0].asnumpy()\n",
    "\n",
    "    def train(self, xval, yval, lr=1e-2):\n",
    "        new_weights_values = empty_val(new_weights)\n",
    "        args = [empty_val(loss)] + [tvm.ndarray.array(xval.astype('float32')), tvm.ndarray.array(yval.astype('float32'))] +\\\n",
    "                [tvm.ndarray.array(np.array(lr).astype('float32'))] + new_weights_values + self.weights_values\n",
    "        training_module(*args)\n",
    "        for wv, new_wv in zip(self.weights_values, new_weights_values):\n",
    "            wv.copyfrom(new_wv)\n",
    "        return args[0].asnumpy()\n",
    "    \n",
    "    def from_keras(self, keras_model):\n",
    "        assert len(keras_model.get_weights()) == len(self.weights_values)\n",
    "        for kv, wv in zip(keras_model.get_weights(), self.weights_values):\n",
    "            if len(kv.shape) == 4:\n",
    "                kv = np.transpose(kv, [3, 2, 0, 1])\n",
    "            elif len(kv.shape) == 2:\n",
    "                kv = np.transpose(kv)\n",
    "\n",
    "            #print(wv.shape, \" <- \", kv.shape)\n",
    "\n",
    "            wv.copyfrom(kv)\n",
    "            \n",
    "    def to_keras(self, keras_model):\n",
    "        assert len(keras_model.get_weights()) == len(self.weights_values)\n",
    "        new_keras_weights = []\n",
    "        for kv, wv in zip(keras_model.get_weights(), self.weights_values):\n",
    "            wv_np = wv.asnumpy()\n",
    "            if len(kv.shape) == 4:\n",
    "                wv_np = np.transpose(wv_np, [2, 3, 1, 0])\n",
    "            elif len(kv.shape) == 2:\n",
    "                wv_np = np.transpose(wv_np)\n",
    "            new_keras_weights.append(wv_np)\n",
    "\n",
    "        keras_model.set_weights(new_keras_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tvm model and copy weights from the keras model as initialization. Check that transferring weights between tvm and keras works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm_model = TvmModel()\n",
    "keras_model_to_check = make_keras_model()\n",
    "\n",
    "tvm_model.from_keras(keras_model)\n",
    "tvm_model.to_keras(keras_model_to_check)\n",
    "\n",
    "for w2, w1 in zip(keras_model_to_check.get_weights(), keras_model.get_weights()):\n",
    "    np.testing.assert_allclose(w2, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on batches should produce the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30751 vs [2.3075097, 0.0625]\n",
      "2.306449 vs [2.3064487, 0.0625]\n",
      "2.304803 vs [2.304803, 0.0625]\n",
      "2.3064039 vs [2.306404, 0.0625]\n",
      "2.3068392 vs [2.3068395, 0.0625]\n",
      "2.3067234 vs [2.3067236, 0.0625]\n",
      "2.306138 vs [2.306138, 0.0625]\n",
      "2.3060918 vs [2.3060918, 0.0625]\n",
      "2.3039417 vs [2.3039412, 0.09375]\n",
      "2.3034418 vs [2.3034415, 0.125]\n"
     ]
    }
   ],
   "source": [
    "for xx, yy in itertools.islice(batches(x_train, y_train), 10):\n",
    "    print(tvm_model.test(xx, yy), \"vs\", keras_model_to_check.test_on_batch(xx, yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare test time on several batches to know what to expect (on my machine keras is about 2x faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 2min 29s, total: 4min 13s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for xx, yy in itertools.islice(batches(x_train, y_train), 1000):\n",
    "    keras_model_to_check.test_on_batch(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 6s, sys: 1min 6s, total: 8min 13s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for xx, yy in itertools.islice(batches(x_train, y_train), 1000):\n",
    "    tvm_model.test(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the reference keras model\n",
    "\n",
    "Let's first train the reference keras model. We use our custom batch generator to make the comparison fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.2226 - acc: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabd84679e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.fit_generator(batches(x_train, y_train), steps_per_epoch=int(len(x_train) / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the tvm model\n",
    "Train the tvm model. Note that train loss here is the loss from the last step, so don't compare it to the keras train loss. On my machine training is about 5x slower than training with keras instead of being just 2x slower as with testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 60000  step: 1874  192ms/step  train loss: 0.0156072350218892178\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "seen = 0\n",
    "for step, (xs, ys) in enumerate(batches(x_train, y_train)):\n",
    "    seen += xs.shape[0]\n",
    "    train_loss = tvm_model.train(xs, ys)\n",
    "    \n",
    "    cur_time = time.time()\n",
    "    \n",
    "    ms_per_step = int(1000*(cur_time - start_time)/(step + 1))\n",
    "    \n",
    "    print(\"samples: {}  step: {}  {}ms/step  train loss: {}\".format(seen, step, ms_per_step, train_loss), end='\\r')\n",
    "        \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We'll test both models using keras, the results are lists `[loss, accuracy]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4949565873696254, 0.8694911858974359]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.evaluate_generator(batches(x_test, y_test), steps=int(len(x_test) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5270105603461465, 0.8519631410256411]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm_model.to_keras(keras_model_to_check)\n",
    "keras_model_to_check.evaluate_generator(batches(x_test, y_test), steps=int(len(x_test) / batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
