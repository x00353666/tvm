{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVM-level automatic differentiation\n",
    "This notebook shows how to use tvm-level automatic differentiation and discusses how it works internally, what you can expect to be differentiated well, and what still requires some more work. Note that this is a work-in-progress and the result of differentiating certain operations is not as performant yet as we want it to be.\n",
    "\n",
    "Let's start by importing modules and defining some helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import topi\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_shape(tensor):\n",
    "    return [tvm.ir_pass.Simplify(s).value for s in tensor.shape]\n",
    "\n",
    "# This function builds a tvm function, runs it for several iterations, \n",
    "# and returns the time of running one iteration in milliseconds\n",
    "def measure_performance(outputs, inputs, min_seconds=1):\n",
    "    sched = tvm.create_schedule([o.op for o in outputs])\n",
    "    mout = tvm.build(sched, outputs + inputs)\n",
    "    \n",
    "    arguments = [tvm.nd.empty(get_shape(t), t.dtype) for t in outputs + inputs]\n",
    "    \n",
    "    seconds = 0\n",
    "    iters = 0\n",
    "    iters_to_do = 1\n",
    "    while seconds < min_seconds:\n",
    "        if seconds > 0:\n",
    "            iters_to_do = min(100, math.ceil(iters / seconds))\n",
    "        before = time.time()\n",
    "        for i in range(iters_to_do):\n",
    "            mout(*arguments)\n",
    "        after = time.time()\n",
    "        seconds += after - before\n",
    "        iters += iters_to_do\n",
    "        \n",
    "    return int(1000*(seconds/iters))\n",
    "\n",
    "# Print the lowered representation\n",
    "def show_lowered(outputs, inputs):\n",
    "    sout = tvm.create_schedule([o.op for o in outputs])\n",
    "    mout = tvm.lower(sout, outputs + inputs, simple_mode=True)\n",
    "    print(mout)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use automatic differentiation\n",
    "Basically, all you need is the function `tvm.ir_pass.JacobianRecursive` which takes a tensor, differentiates it with respect to other given tensors using reverse accumulation, and applies certain optimizations. Let's consider an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "X = tvm.placeholder((32, 10000), name='X')\n",
    "W = tvm.placeholder((3000, 10000), name='W')\n",
    "B = tvm.placeholder((3000,), name='B')\n",
    "\n",
    "# output\n",
    "Y = topi.nn.dense(X, W, B)\n",
    "\n",
    "# Adjoint (head gradients). In this case it is has the same shape as Y and\n",
    "# represents the gradient of some hypothetical scalar loss with respect to Y.\n",
    "# In the most common case Y will be the loss itself with the shape (1,)\n",
    "# and H will simply be a scalar 1, but here we want to look at a more general case.\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "\n",
    "# Get Jacobians of Y wrt W and B, multiplied by H,\n",
    "# in other words, get gradients of some loss wrt W and B\n",
    "# given H, the gradient of this loss wrt Y\n",
    "[dW, dB] = tvm.ir_pass.JacobianRecursive(Y, [W, B], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  858\n",
      "backward 928\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, B, W]))\n",
    "print(\"backward\", measure_performance([dW, dB], [X, B, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works internally\n",
    "Internally `JacobianRecursive` builds tensors of the form close to `matmul(H, Jacobian(Y, W))` where `Jacobian(Y, W)` simply differentiates Y wrt W assuming that Y directly uses W (`JacobianRecursive` doesn't make this assumption). So let's look at the `Jacobian` function. It has additional parameter which indicates whether to perform optimizations, so let's look at an unoptimized result of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origiginal tensor Y:\n",
      "tensor compute{0x271c110}[0] : float32 [32, 3000]\n",
      "axes (i : [0, 31], j : [0, 2999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k : [0, 9999])\n",
      "    condition (uint1)1\n",
      "    source[0] = (X(i, k)*W(j, k))\n",
      "\n",
      "tensor X{0x23c4660}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x23c4660)\n",
      "\n",
      "tensor W{0x22a07e0}[0] : float32 [3000, 10000]\n",
      "    placeholder(W, 0x22a07e0)\n",
      "\n",
      "\n",
      "\n",
      "Jacobian(Y, W):\n",
      "tensor compute.jacobian{0x2386970}[0] : float32 [32, 3000, 3000, 10000]\n",
      "axes (i : [0, 31], j : [0, 2999], jac_i0 : [0, 2999], jac_i1 : [0, 9999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x.der]  rhs [y.der]\n",
      "    combiner [(x.der + y.der)]\n",
      "    axes (k : [0, 9999])\n",
      "    condition (uint1)1\n",
      "    source[0] = (X(i, k)*float32(((jac_i0 == j) && (jac_i1 == k))))\n",
      "\n",
      "tensor X{0x23c4660}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x23c4660)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = topi.nn.dense(X, W)\n",
    "dYdW = tvm.ir_pass.Jacobian(Y, W, False)\n",
    "\n",
    "# This function prints out a tensor with all its dependencies in a slightly more readable\n",
    "# format, in particular, it prints every attribute of a reduction on a new line\n",
    "print(\"The origiginal tensor Y:\")\n",
    "print(tvm.PrintTensorRecursively(Y))\n",
    "print(\"\\nJacobian(Y, W):\")\n",
    "print(tvm.PrintTensorRecursively(dYdW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `W(j, k)` in the original tensor Y became `float32(((jac_i0 == j) && (jac_i1 == k)))` in the Jacobian, which is the derivative of `W(j, k)` wrt `W(jac_i0, jac_i1)` (it's equal to 1 if the corresponding indices coincide, otherwise it's zero). Of course, computing this Jacobian is very inefficient, because it consists of summing over mostly zero values, so it should be optimized by propagating the information that `jac_i1 == k` and completely removing the summation. It may be done with the function `OptimizeAndLiftNonzeronessConditions` (which is called by the `Jacobian` function by default). Let's call it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor compute.jacobian{0x2363500}[0] : float32 [32, 3000, 3000, 10000]\n",
      "axes (i : [0, 31], j : [0, 2999], jac_i0 : [0, 2999], jac_i1 : [0, 9999])\n",
      "    select(((((((jac_i1 <= 9999) && (i <= 31)) && (j == jac_i0)) && (j <= 2999)) && (jac_i0 <= 2999)) && (jac_i1 <= 9999)), (X(i, jac_i1)*1.000000f), 0.000000f)\n",
      "\n",
      "tensor X{0x23c4660}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x23c4660)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dYdW_optimized = tvm.ir_pass.OptimizeAndLiftNonzeronessConditions(dYdW)\n",
    "print(tvm.PrintTensorRecursively(dYdW_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction was eliminated completely, and replaced with a conditional expression returning `X(i, jac_i1)` if `j == jac_i0` and 0 otherwise. You can see a small deficiency here: the condition contains redundant formulas which are implied by variable boundaries (they are usually eliminated in subsequent passes though).\n",
    "\n",
    "The condition `j == jac_i0` may be used to eliminate another reduction. Recall that the Jacobian is used in a formula looking similar to `matmul(H, Jacobian(Y, W))`, so the reduction to be eliminated is a summation used in matrix multiplication. To perform this transformation, `JacobianRecursive` inlines the Jacobian and calls `OptimizeAndLiftNonzeronessConditions` once more. Let's do this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor tensor{0x23ef840}[0] : float32 [3000, 10000]\n",
      "axes (ax0 : [0, 2999], ax1 : [0, 9999])\n",
      "    select(((ax0 <= 2999) && (ax1 <= 9999)), auto.extracted_reduction(ax0, ax1), 0.000000f)\n",
      "\n",
      "tensor auto.extracted_reduction{0x248b4d0}[0] : float32 [3000, 10000]\n",
      "axes (ax0 : [0, 2999], ax1 : [0, 9999])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k0.shifted : [0, 31])\n",
      "    condition (k0.shifted <= 31)\n",
      "    source[0] = (H(k0.shifted, ax0)*(X(k0.shifted, ax1)*1.000000f))\n",
      "\n",
      "tensor H{0x2212a80}[0] : float32 [32, 3000]\n",
      "    placeholder(H, 0x2212a80)\n",
      "\n",
      "tensor X{0x23c4660}[0] : float32 [32, 10000]\n",
      "    placeholder(X, 0x23c4660)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generalized matmul works with tensors of arbitrary dimensions and takes\n",
    "# an additional parameter: the number of dimensions to contract. It is\n",
    "# semantically equivalent to reshaping into two matrices, \n",
    "# performing matrix multiplication, and then reshaping back\n",
    "dLdW = tvm.generalized_matmul(H, dYdW_optimized, 2)\n",
    "\n",
    "# We have to inline dYdW_optimized because OptimizeAndLiftNonzeronessConditions works\n",
    "# only with a single tensor\n",
    "dLdW_inlined = tvm.ir_pass.InlineNonReductions(dLdW, [dYdW_optimized])\n",
    "\n",
    "# Perform the main optimization\n",
    "dLdW_optimized = tvm.ir_pass.OptimizeAndLiftNonzeronessConditions(dLdW_inlined)\n",
    "print(tvm.PrintTensorRecursively(dLdW_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now there is only one reduction axis left, an there no comparison `j == jac_i0` anymore. You can also see another quirk: `OptimizeAndLiftNonzeronessConditions` has split our tensor into two parts: one just contains a condition (quite useless), and another is a reduction. The reason is that this function (as you can guess from its name) lifts nonzeroness conditions up, and sometimes it factors them out of a reduction, and because of some tvm restrictions the reduction must be extracted into a separate tensor. (Note that in the case of summation the condition could be put into the `condition` field of the reduction, but this wouldn't work for different reductions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supported operations\n",
    "Here is a list of operations which seem to be differentiated quite well by our autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 1000), name='X')\n",
    "W = tvm.placeholder((1000, 1000), name='W')\n",
    "B = tvm.placeholder((1000,), name='B')\n",
    "\n",
    "Y = topi.nn.dense(X, W, B)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X, W, B], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  28\n",
      "backward 60\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, B, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, B, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 17, 28, 28), name='X')\n",
    "W = tvm.placeholder((19, 17, 3, 3), name='W')\n",
    "Y = topi.nn.conv2d(X, W, [1, 1], [0, 0])\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X, W], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  52\n",
      "backward 118\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somewhat supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average pooling\n",
    "The performance is suspicious but the generated code looks ok except for large if expressions which cannot be eliminated by subsequent passes (the problem in not nearly as horrible as with max pooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 17, 280, 280), name='X')\n",
    "Y = topi.nn.pool(X, [2, 2], [2, 2], [0, 0, 0, 0], 'avg')\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  14\n",
      "backward 233\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor tensor.grad{0x2d73500}[0] : float32 [32, 17, 280, 280]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 16], ax2 : [0, 279], ax3 : [0, 279])\n",
      "    select(((((((((((((((((ax0 <= 31) && (ax1 <= 16)) && (ax2 <= 279)) && (ax3 <= 279)) && ((ax2 - (((ax2 + -1)/2)*2)) <= 3)) && (-1 <= ax2)) && (((((ax2 + -1)/2)*2) - ax2) <= 0)) && (0 <= ax2)) && (ax2 <= 280)) && (ax2 <= 279)) && ((ax3 - (((ax3 + -1)/2)*2)) <= 3)) && (-1 <= ax3)) && (((((ax3 + -1)/2)*2) - ax3) <= 0)) && (0 <= ax3)) && (ax3 <= 280)) && (ax3 <= 279)), auto.extracted_reduction(ax0, ax1, ax2, ax3), 0.000000f)\n",
      "\n",
      "tensor auto.extracted_reduction{0x27e8dd0}[0] : float32 [32, 17, 280, 280]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 16], ax2 : [0, 279], ax3 : [0, 279])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k3.shifted : [0, 1], k2.shifted : [0, 1])\n",
      "    condition (((((((((ax2 - 1) <= ((k2.shifted*2) + (((ax2 - 1)/2)*2))) && ((0 - ((ax2 - 1)/2)) <= k2.shifted)) && (((k2.shifted*2) + (((ax2 - 1)/2)*2)) <= ax2)) && ((k2.shifted + ((ax2 - 1)/2)) <= 139)) && ((ax3 - 1) <= ((k3.shifted*2) + (((ax3 - 1)/2)*2)))) && ((0 - ((ax3 - 1)/2)) <= k3.shifted)) && (((k3.shifted*2) + (((ax3 - 1)/2)*2)) <= ax3)) && ((k3.shifted + ((ax3 - 1)/2)) <= 139))\n",
      "    source[0] = (H(ax0, ax1, (k2.shifted + ((ax2 - 1)/2)), (k3.shifted + ((ax3 - 1)/2)))*(1.000000f*0.250000f))\n",
      "\n",
      "tensor H{0x2480340}[0] : float32 [32, 17, 140, 140]\n",
      "    placeholder(H, 0x2480340)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.PrintTensorRecursively(grads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax (homebrewn)\n",
    "Softmax from topi causes performance problems (see below), but we can write our own softmax which works better but still not perfectly (seems like some performance problems when used after other layers like dense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((60, 100), name=\"X\")\n",
    "W = tvm.placeholder((1000, 1000), name='W')\n",
    "\n",
    "exps = topi.exp(topi.nn.dense(X, W))\n",
    "sumexps = topi.sum(exps, axis=-1, keepdims=True)\n",
    "Y = exps/sumexps\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X, W], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  10\n",
      "backward 223\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X, W]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, W, H]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completely unsupported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "Flatten uses the division and modulo operations which are not well supported by our zero-eliminating transformations. A related problem is [issue 1711](https://github.com/dmlc/tvm/issues/1711)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((32, 10, 20, 25), name='X')\n",
    "Y = topi.nn.flatten(X)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  0\n",
      "backward 1568\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor compute.grad{0x26ae2a0}[0] : float32 [32, 10, 20, 25]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 9], ax2 : [0, 19], ax3 : [0, 24])\n",
      "    select(((((ax0 <= 31) && (ax1 <= 9)) && (ax2 <= 19)) && (ax3 <= 24)), auto.extracted_reduction(ax0, ax1, ax2, ax3), 0.000000f)\n",
      "\n",
      "tensor auto.extracted_reduction{0x2716d20}[0] : float32 [32, 10, 20, 25]\n",
      "axes (ax0 : [0, 31], ax1 : [0, 9], ax2 : [0, 19], ax3 : [0, 24])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k1.shifted : [0, 4999])\n",
      "    condition ((((k1.shifted <= 4999) && (ax1 == ((k1.shifted/500) % 10))) && (ax2 == ((k1.shifted/25) % 20))) && (ax3 == (k1.shifted % 25)))\n",
      "    source[0] = (H(ax0, k1.shifted)*1.000000f)\n",
      "\n",
      "tensor H{0x2d73890}[0] : float32 [32, 5000]\n",
      "    placeholder(H, 0x2d73890)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.PrintTensorRecursively(grads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the compiler has to figure out that `k1.shifted` is directly expressible using `ax1, ax2, ax3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling\n",
    "Reducing with other combiners, like max, is a bit trickier than summation. Currently reducing with max is partially supported: it can be differentiated, and most of transformations, like moving some conditions out of the resulting reduction, work, but still there are some transformations left to be implemented to make it performant. In particular, autodifferentiated max pooling cannot even be compiled in most practical cases, because it involves creating tensors larger than 2^31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((1, 2, 100, 100), name='X')\n",
    "Y = topi.nn.pool(X, [2, 2], [2, 2], [0, 0, 0, 0], 'max')\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  0\n",
      "backward 475\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor tensor.grad{0x24ae0b0}[0] : float32 [1, 2, 100, 100]\n",
      "axes (ax0 : [0, 0], ax1 : [0, 1], ax2 : [0, 99], ax3 : [0, 99])\n",
      "    select(((((((((((((((((ax0 == 0) && (ax1 <= 1)) && (ax2 <= 99)) && (ax3 <= 99)) && ((ax2 - (((ax2 + -1)/2)*2)) <= 3)) && (-1 <= ax2)) && (((((ax2 + -1)/2)*2) - ax2) <= 0)) && (0 <= ax2)) && (ax2 <= 100)) && (ax2 <= 99)) && ((ax3 - (((ax3 + -1)/2)*2)) <= 3)) && (-1 <= ax3)) && (((((ax3 + -1)/2)*2) - ax3) <= 0)) && (0 <= ax3)) && (ax3 <= 100)) && (ax3 <= 99)), auto.extracted_reduction(ax0, ax1, ax2, ax3), 0.000000f)\n",
      "\n",
      "tensor auto.extracted_reduction{0x2481f30}[0] : float32 [1, 2, 100, 100]\n",
      "axes (ax0 : [0, 0], ax1 : [0, 1], ax2 : [0, 99], ax3 : [0, 99])\n",
      "Reduction\n",
      "    identity [0.000000f]\n",
      "    lhs [x]  rhs [y]\n",
      "    combiner [(x + y)]\n",
      "    axes (k3.shifted : [0, 1], k2.shifted : [0, 1])\n",
      "    condition (((((((((ax2 - 1) <= ((k2.shifted*2) + (((ax2 - 1)/2)*2))) && ((0 - ((ax2 - 1)/2)) <= k2.shifted)) && (((k2.shifted*2) + (((ax2 - 1)/2)*2)) <= ax2)) && ((k2.shifted + ((ax2 - 1)/2)) <= 49)) && ((ax3 - 1) <= ((k3.shifted*2) + (((ax3 - 1)/2)*2)))) && ((0 - ((ax3 - 1)/2)) <= k3.shifted)) && (((k3.shifted*2) + (((ax3 - 1)/2)*2)) <= ax3)) && ((k3.shifted + ((ax3 - 1)/2)) <= 49))\n",
      "    source[0] = (H(0, ax1, (k2.shifted + ((ax2 - 1)/2)), (k3.shifted + ((ax3 - 1)/2)))*auto.extracted_reduction(0, ax1, (k2.shifted + ((ax2 - 1)/2)), (k3.shifted + ((ax3 - 1)/2)), ax0, ax1, ax2, ax3))\n",
      "\n",
      "tensor H{0x27df8c0}[0] : float32 [1, 2, 50, 50]\n",
      "    placeholder(H, 0x27df8c0)\n",
      "\n",
      "tensor auto.extracted_reduction{0x249c710}[0] : float32 [1, 2, 50, 50, 1, 2, 100, 100]\n",
      "axes (ax0 : [0, 0], ax1 : [0, 1], ax2 : [0, 49], ax3 : [0, 49], jac_i0 : [0, 0], jac_i1 : [0, 1], jac_i2 : [0, 99], jac_i3 : [0, 99])\n",
      "Reduction\n",
      "    identity [0.000000f, -340282346638528859811704183484516925440.000000f]\n",
      "    lhs [x.der, x]  rhs [y.der, y]\n",
      "    combiner [((x.der*select((x < y), 0.000000f, 1.000000f)) + (y.der*select((x < y), 1.000000f, 0.000000f))), max(x, y)]\n",
      "    axes (rv.shifted : [0, 1], rv.shifted : [0, 1])\n",
      "    condition ((rv.shifted <= 1) && (rv.shifted <= 1))\n",
      "    source[0] = select(((((ax3*2) + rv.shifted) == jac_i3) && ((rv.shifted + (ax2*2)) == jac_i2)), 1.000000f, 0.000000f)\n",
      "    source[1] = X(ax0, ax1, ((ax2*2) + rv.shifted), ((ax3*2) + rv.shifted))\n",
      "\n",
      "tensor X{0x23b9c20}[0] : float32 [1, 2, 100, 100]\n",
      "    placeholder(X, 0x23b9c20)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.PrintTensorRecursively(grads[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last tensor we can see that it must be zero for most values of `jac_i2` and `jac_i3`. They can actually be replaced with `jac_i2 - ax2*2` and `jac_i3 - ax3*2` which can be proved to have much smaller range. A similar transformation is successfully performed in the average pooling case, but here the situation is slightly different causing it to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "Softmax uses max behind the scenes, causing some performance problems. We havent't yet investingated into it though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tvm.placeholder((60, 200), name=\"X\")\n",
    "Y = topi.nn.softmax(X)\n",
    "\n",
    "H = tvm.placeholder(Y.shape, name='H')\n",
    "grads = tvm.ir_pass.JacobianRecursive(Y, [X], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  0\n",
      "backward 785\n"
     ]
    }
   ],
   "source": [
    "print(\"forward \", measure_performance([Y], [X]))\n",
    "print(\"backward\", measure_performance(list(grads), [X, H]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
